{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning for Wine Classification\n",
    "\n",
    "In this notebook, we will apply **ensemble learning** to classify the wine dataset using multiple classifiers. We'll use a **Voting Classifier**, which combines three different models: **RandomForestClassifier**, **GradientBoostingClassifier**, and **LogisticRegression**. This method aims to improve classification performance by leveraging the strengths of each individual model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the Wine Dataset\n",
    "First, we will load the wine dataset, which contains information about various wine features and their corresponding class (wine types). We'll assume the dataset is stored in `Datasets/wine_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Wine dataset from the Datasets folder\n",
    "# Assuming the wine data is in a CSV format called \"wine_data.csv\" stored in the \"Datasets\" folder\n",
    "wine_data = pd.read_csv('Datasets/wine_data.csv')\n",
    "\n",
    "# Check the first few rows of the data\n",
    "wine_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data Preprocessing\n",
    "Next, we will separate the features (X) and target variable (y), and then split the dataset into training and test sets. The target variable is assumed to be the `Class` column, which represents the wine type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target variable (y)\n",
    "X = wine_data.drop(columns=['Class'])  # Features\n",
    "y = wine_data['Class']  # Target variable (wine types)\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Standardize the Features\n",
    "We will scale the features to standardize the data, ensuring that each feature has a mean of 0 and a standard deviation of 1. This helps certain classifiers perform better, especially **Logistic Regression** and **Gradient Boosting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features for better performance\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define Base Models\n",
    "We will define three base learners: **RandomForestClassifier**, **GradientBoostingClassifier**, and **LogisticRegression**. These models will be combined in the next step using a **VotingClassifier**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base learners (models)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create and Train the Voting Classifier\n",
    "Now, we will create a **VotingClassifier** that combines the three base models. The final prediction will be based on the majority vote from these classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the Voting Classifier (ensemble method)\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('rf', rf),\n",
    "    ('gb', gb),\n",
    "    ('lr', lr)\n",
    "], voting='hard')  # 'hard' voting uses the majority class from the base learners\n",
    "\n",
    "# Train the ensemble model on the training data\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Make Predictions and Evaluate the Model\n",
    "After training the model, we will make predictions on the test set and evaluate the performance using accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accuracy of the Ensemble Learning Model: 0.98"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "stream"
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy of the Ensemble Learning Model: {accuracy:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
